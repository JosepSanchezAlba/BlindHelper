<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <title>YOLOv8 OpenImages (Depuraci칩n)</title>
  <style>
    body { font-family: sans-serif; text-align: center; }
    canvas { border: 2px solid black; margin-top: 10px; }
    select, button { font-size: 1rem; padding: 5px; margin: 5px; }
  </style>
</head>
<body>
  <h1>YOLOv8 OpenImages (Depuraci칩n)</h1>

  <select id="cameraSelect"></select>
  <button onclick="startCamera()">Iniciar</button>

  <br />
  <canvas id="canvas" width="640" height="480"></canvas>
  <p id="status">Cargando modelo...</p>

  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <script>
    let videoStream, session, video, classNames = [];
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');

    async function loadClassNames() {
      const response = await fetch('classes.txt');
      const text = await response.text();
      classNames = text.split('\n').map(l => l.trim()).filter(l => l.length);
      console.log("Clases cargadas:", classNames.length, classNames.slice(0, 10));
    }

    async function setupModel() {
      session = await ort.InferenceSession.create('./yolov8s-oiv7.onnx');
      document.getElementById("status").textContent = "Modelo cargado. Selecciona c치mara.";
    }

    async function listCameras() {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const videoDevices = devices.filter(d => d.kind === "videoinput");
      const select = document.getElementById("cameraSelect");
      videoDevices.forEach((device) => {
        const option = document.createElement("option");
        option.value = device.deviceId;
        option.text = device.label || `C치mara ${select.length + 1}`;
        select.appendChild(option);
      });
    }

    async function startCamera() {
      const deviceId = document.getElementById("cameraSelect").value;
      if (videoStream) videoStream.getTracks().forEach(t => t.stop());

      videoStream = await navigator.mediaDevices.getUserMedia({
        video: { deviceId: { exact: deviceId } },
        audio: false
      });

      if (!video) {
        video = document.createElement('video');
        video.setAttribute('autoplay', '');
        video.setAttribute('playsinline', '');
      }

      video.srcObject = videoStream;
      await video.play();
      detectLoop();
    }

    function preprocessFrame() {
      const tmpCanvas = document.createElement('canvas');
      tmpCanvas.width = 640;
      tmpCanvas.height = 640;
      const tmpCtx = tmpCanvas.getContext('2d');
      tmpCtx.drawImage(video, 0, 0, 640, 640);
      const imageData = tmpCtx.getImageData(0, 0, 640, 640);

      const data = new Float32Array(3 * 640 * 640);
      for (let i = 0; i < 640 * 640; i++) {
        data[i] = imageData.data[i * 4] / 255;               // R
        data[i + 640 * 640] = imageData.data[i * 4 + 1] / 255; // G
        data[i + 2 * 640 * 640] = imageData.data[i * 4 + 2] / 255; // B
      }

      return new ort.Tensor("float32", data, [1, 3, 640, 640]);
    }

    async function detectLoop() {
      if (!video || video.readyState !== 4) return requestAnimationFrame(detectLoop);

      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      const inputTensor = preprocessFrame();

      try {
        const outputMap = await session.run({ images: inputTensor });
        const output = outputMap[session.outputNames[0]].data;

        drawDetections(output);
      } catch (err) {
        console.error("Error de inferencia:", err);
      }

      requestAnimationFrame(detectLoop);
    }

    function drawDetections(data) {
      ctx.lineWidth = 2;
      ctx.font = "14px Arial";
      ctx.textBaseline = "top";

      for (let i = 0; i < data.length; i += 6) {
        const [batch, classId, score, x1, y1, x2, y2] = data.slice(i, i + 7);
        if (score < 0.5) continue;

        const label = classNames[classId] || `class_${classId}`;
        const x = x1 * canvas.width / 640;
        const y = y1 * canvas.height*
